{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gymnasium\n",
    "from datetime import date\n",
    "import pathlib\n",
    "\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_cfg= {\n",
    "        \"action_space\":[-1, 1],\n",
    "        \"obs_space\":[-10, 10],\n",
    "        \"num\":[1],\n",
    "        \"den\":[1, 10, 20],\n",
    "        \"x_0\":[0],\n",
    "        \"dt\":0.01,\n",
    "        \"y_0\":0,\n",
    "        \"t_0\":0,\n",
    "        \"t_end\":5,\n",
    "        \"y_ref\":1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Is Installed via pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.registration import register\n",
    "import gym_control\n",
    "\n",
    "register(\n",
    "    id='gym_control/LinearSISOEnv-v0',\n",
    "    entry_point='gym_control.envs:LinearSISOEnv',\n",
    "    max_episode_steps=3000)\n",
    "\n",
    "env = gymnasium.make(\"gym_control/LinearSISOEnv-v0\",env_config=env_cfg)\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_control.envs import LinearSISOEnv\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return LinearSISOEnv(env_config)\n",
    "register_env(\"LinearSISOEnv\", lambda config: LinearSISOEnv(env_cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init RAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reward(train_result:list):\n",
    "    plt.figure(1)\n",
    "    plt.title(\"Rewards\")\n",
    "    plt.plot(np.arange(1,len(train_result[\"hist_stats\"][\"episode_reward\"])+1,1),train_result[\"hist_stats\"][\"episode_reward\"],'-x')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with PPO Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms import ppo\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "#### Simple way\n",
    "# algo = ppo.PPO(env=LinearSISOEnv,config={\"env_config\":env_cfg})\n",
    "# result = algo.train()\n",
    "\n",
    "\n",
    "# More complex/customizable way of training in rllib\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .rollouts(num_rollout_workers=0)\n",
    "    .resources(num_gpus=1)\n",
    "    .environment(env=LinearSISOEnv,env_config=env_cfg)\n",
    "    .training()\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(4):\n",
    "    result = algo.train()\n",
    "    print(pretty_print(result))\n",
    "\n",
    "    if i % 2 == 0:\n",
    "        checkpoint_dir = algo.save()\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")\n",
    "\n",
    "plot_reward(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with DDPG Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms import ddpg\n",
    "algo_ddpg = ddpg.DDPG(env=LinearSISOEnv,config={\"env_config\":env_cfg})\n",
    "# Train ddpg one iteration\n",
    "result_ddpg = algo_ddpg.train()\n",
    "plot_reward(result_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ddpg import DDPGConfig\n",
    "\n",
    "algo = (\n",
    "    DDPGConfig()\n",
    "    .rollouts(num_rollout_workers=0)\n",
    "    .resources(num_gpus=1)\n",
    "    .environment(env=LinearSISOEnv,env_config=env_cfg)\n",
    "    .training()\n",
    "    .build()\n",
    ")\n",
    "\n",
    "\n",
    "algo_name = \"DDPG\"\n",
    "now = datetime.now()\n",
    "checkpoint_dir = f\"{algo_name}_{now.day}_{now.month}_{now.year}_{now.hour}_{now.minute}_{now.second}\"\n",
    "checkpoint_dir = str(pathlib.PurePath(\"ray_results\",checkpoint_dir))\n",
    "\n",
    "best_rew = -99999\n",
    "eps_rewards = []\n",
    "best_ckpt = \"\"\n",
    "for i in range(10):\n",
    "    result = algo.train()\n",
    "    print(result[\"episode_reward_mean\"])\n",
    "    eps_rewards.append(result[\"episode_reward_mean\"])\n",
    "    if result[\"episode_reward_mean\"] > best_rew:\n",
    "        best_rew = result[\"episode_reward_mean\"]\n",
    "        ckpt_dir = algo.save(checkpoint_dir=checkpoint_dir)\n",
    "        best_ckpt = ckpt_dir\n",
    "        print(f\"Best checkpoint saved in directory {ckpt_dir}\")\n",
    "    if result[\"episode_reward_mean\"]>= -1.1:\n",
    "        break\n",
    "\n",
    "algo.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "obs, info = env.reset()\n",
    "torch.from_numpy(obs).float().to(\"cuda\")\n",
    "torch.from_numpy(obs).cuda(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = algo.get_policy()\n",
    "policy.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "best_ckpt = \"ray_results\\\\DDPG_10_7_2023_1_54_48\\\\checkpoint_000003\"\n",
    "algo_trained = Algorithm.from_checkpoint(best_ckpt)\n",
    "algo_trained\n",
    "\n",
    "\n",
    "obs, info = env.reset()\n",
    "\n",
    "\n",
    "for x in range(100):\n",
    "    action = algo_trained.compute_single_action(obs)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Ray[Tune]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import air, tune\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.air.config import CheckpointConfig\n",
    "from ray.rllib.algorithms.ddpg import DDPGConfig\n",
    "\n",
    "config = DDPGConfig().training(lr=0.1)\n",
    "config.environment(env=LinearSISOEnv,env_config=env_cfg)\n",
    "config.rollouts(num_rollout_workers=1)\n",
    "config.framework(\"torch\")\n",
    "config.resources(num_gpus=1)\n",
    "\n",
    "\n",
    "scheduler = AsyncHyperBandScheduler(time_attr=\"training_iteration\")\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    \"DDPG\",\n",
    "    run_config=air.RunConfig(\n",
    "        stop={\"episode_reward_mean\": -1.1},\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            checkpoint_frequency=1,\n",
    "            checkpoint_at_end=True)\n",
    "    ),\n",
    "    param_space=config,\n",
    "    # # For 3 iteration if any progress does not happen in the metric, stop exp\n",
    "    # tune_config=tune.TuneConfig(\n",
    "    #         scheduler=scheduler, num_samples=3, metric=\"episode_reward_mean\", mode=\"max\"),\n",
    "\n",
    ")\n",
    "\n",
    "results_ddpg_tuner = tuner.fit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = results_ddpg_tuner.get_best_result(metric=\"episode_reward_mean\", mode=\"max\")\n",
    "# Get the best checkpoint corresponding to the best result.\n",
    "best_checkpoint = best_result.checkpoint\n",
    "best_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Trained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "ddpg_trained = Algorithm.from_checkpoint(\"C:\\\\Users\\\\Furka\\\\ray_results\\\\DDPG\\\\DDPG_LinearSISOEnv_65c58_00000_0_2023-07-09_23-51-37\\\\checkpoint_000012\")\n",
    "ddpg_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlcontrol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
